{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Streamlit_Test_Divinus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1-jdYPD0NyZ"
      },
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q pyngrok\n",
        "!pip install -q streamlit_folium"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkt4iYrO-sOV"
      },
      "source": [
        "!sudo apt-get install python3-dev libmysqlclient-dev > /dev/null\n",
        "!pip install mysqlclient > /dev/null\n",
        "!sudo pip3 install -U sql_magic > /dev/null\n",
        "!pip install psycopg2-binary > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbYvaz-lqqWE",
        "outputId": "f2f4ddd4-e30c-40f4-e2a8-49b9d120cb7a"
      },
      "source": [
        "!pip3 install -U geopandas fiona shapely pyproj geopy pysal descartes\n",
        "#!mkdir data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: geopandas in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already up-to-date: fiona in /usr/local/lib/python3.7/dist-packages (1.8.19)\n",
            "Requirement already up-to-date: shapely in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already up-to-date: pyproj in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already up-to-date: geopy in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already up-to-date: pysal in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already up-to-date: descartes in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: munch in /usr/local/lib/python3.7/dist-packages (from fiona) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from fiona) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy) (1.50)\n",
            "Requirement already satisfied, skipping upgrade: giddy>=2.3.3 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.3.3)\n",
            "Requirement already satisfied, skipping upgrade: mgwr>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: spaghetti>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.5.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.26 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.26.4)\n",
            "Requirement already satisfied, skipping upgrade: spopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from pysal) (0.1.1)\n",
            "Requirement already satisfied, skipping upgrade: inequality>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: mapclassify>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: spvcm>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: spreg>=1.2.2 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.2.2)\n",
            "Requirement already satisfied, skipping upgrade: spglm>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.7/dist-packages (from pysal) (3.6.4)\n",
            "Requirement already satisfied, skipping upgrade: tobler>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: splot>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: access>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: coverage in /usr/local/lib/python3.7/dist-packages (from pysal) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pytest-cov in /usr/local/lib/python3.7/dist-packages (from pysal) (2.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pointpats>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: libpysal>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: segregation>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: esda>=2.3.6 in /usr/local/lib/python3.7/dist-packages (from pysal) (2.3.6)\n",
            "Requirement already satisfied, skipping upgrade: spint>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from pysal) (1.0.7)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from descartes) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: quantecon>=0.4.7 in /usr/local/lib/python3.7/dist-packages (from giddy>=2.3.3->pysal) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from giddy>=2.3.3->pysal) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: rtree in /usr/local/lib/python3.7/dist-packages (from spaghetti>=1.5.6->pysal) (0.9.7)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from spopt>=0.1.1->pysal) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pulp in /usr/local/lib/python3.7/dist-packages (from spopt>=0.1.1->pysal) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from spopt>=0.1.1->pysal) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.7/dist-packages (from spvcm>=0.3.0->pysal) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pysal) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->pysal) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->pysal) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pysal) (8.7.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pysal) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pygeos in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (0.9)\n",
            "Requirement already satisfied, skipping upgrade: rasterio in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (1.2.3)\n",
            "Requirement already satisfied, skipping upgrade: rasterstats in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (0.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: statsmodels in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from tobler>=0.6.0->pysal) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: opencv-contrib-python>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from pointpats>=2.2.0->pysal) (4.5.2.52)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from libpysal>=4.4.0->pysal) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from libpysal>=4.4.0->pysal) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from libpysal>=4.4.0->pysal) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->descartes) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->descartes) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->descartes) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.7/dist-packages (from quantecon>=0.4.7->giddy>=2.3.3->pysal) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: numba>=0.38 in /usr/local/lib/python3.7/dist-packages (from quantecon>=0.4.7->giddy>=2.3.3->pysal) (0.51.2)\n",
            "Requirement already satisfied, skipping upgrade: amply>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from pulp->spopt>=0.1.1->pysal) (0.1.4)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->spopt>=0.1.1->pysal) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: affine in /usr/local/lib/python3.7/dist-packages (from rasterio->tobler>=0.6.0->pysal) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio->tobler>=0.6.0->pysal) (1.4.7)\n",
            "Requirement already satisfied, skipping upgrade: simplejson in /usr/local/lib/python3.7/dist-packages (from rasterstats->tobler>=0.6.0->pysal) (3.17.2)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->tobler>=0.6.0->pysal) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->libpysal>=4.4.0->pysal) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->libpysal>=4.4.0->pysal) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->libpysal>=4.4.0->pysal) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->quantecon>=0.4.7->giddy>=2.3.3->pysal) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.38->quantecon>=0.4.7->giddy>=2.3.3->pysal) (0.34.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.3 in /usr/local/lib/python3.7/dist-packages (from amply>=0.1.2->pulp->spopt>=0.1.1->pysal) (0.17.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9Eafq7SUpn"
      },
      "source": [
        "#https://www.seas.upenn.edu/~joao/nyc-neighborhood.geojson\n",
        "#data hosted here\n",
        "#sedoc own hosted\n",
        "#!curl 'http://www.seas.upenn.edu/~joao/nyc-neighborhood.geojson' -o nyc-neighborhood.geojson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv9Nd7Vt21WS",
        "outputId": "61abebee-6e03-42c9-f7cc-6b30bf78227b"
      },
      "source": [
        "%%writefile test.py\n",
        "from sqlalchemy import create_engine\n",
        "import folium, pandas as pd \n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "conn_string_realEstate = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "    user='Divinus', \n",
        "    password='M1ZpoHfNx9U=', \n",
        "    host = 'jsedocc7.scrc.nyu.edu', \n",
        "    port=3306, \n",
        "    db='divinus',\n",
        "    encoding = 'utf-8'\n",
        ")\n",
        "engine_buildings = create_engine(conn_string_realEstate)\n",
        "\n",
        "#%reload_ext sql_magic\n",
        "#%config SQL.conn_name = 'engine_buildings'\n",
        "\n",
        "query1 = '''\n",
        "SELECT * FROM buildings_predicted_vs_actual\n",
        "'''\n",
        "\n",
        "query2 = '''\n",
        "SELECT * FROM rentals_pred_vs_actual\n",
        "'''\n",
        "\n",
        "query3 = '''\n",
        "SELECT * FROM units_pred_vs_actual\n",
        "'''\n",
        "df_buildingsPrediction = pd.read_sql(query1, con=engine_buildings).dropna()\n",
        "df_rentalsPredictions = pd.read_sql(query2, con=engine_buildings).dropna()\n",
        "df_unitsPredictions = pd.read_sql(query3, con=engine_buildings).dropna()\n",
        "\n",
        "def priceDiffCreator(df):\n",
        "\n",
        "  priceDiffList = []\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    priceDiffList.append(row['Predicted Price'] - row['Actual Price'])\n",
        "\n",
        "  df['Price Difference'] = priceDiffList\n",
        "\n",
        "priceDiffCreator(df_buildingsPrediction)\n",
        "df_buildingsPrediction = df_buildingsPrediction.rename(columns={'Zip' : 'Zipcode'})\n",
        "priceDiffCreator(df_rentalsPredictions)\n",
        "priceDiffCreator(df_unitsPredictions)\n",
        "\n",
        "df_rentalsPredictions = df_rentalsPredictions[df_rentalsPredictions.Latitude >= 40.6]\n",
        "df_rentalsPredictions = df_rentalsPredictions[df_rentalsPredictions.Latitude <= 40.9]\n",
        "df_rentalsPredictions = df_rentalsPredictions[df_rentalsPredictions.Longitude <= -73.90]\n",
        "df_rentalsPredictions = df_rentalsPredictions[df_rentalsPredictions.Longitude >= -74.06]\n",
        "\n",
        "df_unitsPredictions = df_unitsPredictions[df_unitsPredictions.Latitude >= 40.6]\n",
        "df_unitsPredictions = df_unitsPredictions[df_unitsPredictions.Latitude <= 40.9]\n",
        "df_unitsPredictions = df_unitsPredictions[df_unitsPredictions.Longitude <= -73.90]\n",
        "df_unitsPredictions = df_unitsPredictions[df_unitsPredictions.Longitude >= -74.06]\n",
        "\n",
        "def zipcodeGetter(df):\n",
        "  zips = []\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    if row['Zipcode'] not in zips:\n",
        "      zips.append(row['Zipcode'])\n",
        "\n",
        "  return zips\n",
        "\n",
        "def avgPriceDiffZip(zips, df):\n",
        "  avgPriceDiffList = []\n",
        "\n",
        "  for zipcode in zips:\n",
        "    counter = 0\n",
        "    totalPriceDiff = 0\n",
        "    for index, row in df.iterrows():\n",
        "      if row['Zipcode'] == zipcode:\n",
        "        counter += 1\n",
        "        totalPriceDiff += row['Price Difference']\n",
        "\n",
        "    avgPriceDiffList.append(round(totalPriceDiff / counter, 2))\n",
        "  return avgPriceDiffList\n",
        "\n",
        "zipsBuildings = zipcodeGetter(df_buildingsPrediction)\n",
        "zipsUnits = zipcodeGetter(df_unitsPredictions)\n",
        "zipsRentals = zipcodeGetter(df_rentalsPredictions) \n",
        "\n",
        "avgPriceDiffListBuildings = avgPriceDiffZip(zipsBuildings, df_buildingsPrediction)\n",
        "avgPriceDiffListUnits = avgPriceDiffZip(zipsUnits, df_unitsPredictions)\n",
        "avgPriceDiffListRentals = avgPriceDiffZip(zipsRentals, df_rentalsPredictions)\n",
        "\n",
        "dataBuildings = {'Zipcodes' : zipsBuildings, 'Average Price Difference' : avgPriceDiffListBuildings}\n",
        "dataUnits = {'Zipcodes' : zipsUnits, 'Average Price Difference' : avgPriceDiffListUnits}\n",
        "dataRentals = {'Zipcodes' : zipsRentals, 'Average Price Difference' : avgPriceDiffListRentals}\n",
        "\n",
        "df_cleanBuildings = pd.DataFrame(dataBuildings)\n",
        "df_cleanUnits = pd.DataFrame(dataUnits)\n",
        "df_cleanRentals = pd.DataFrame(dataRentals)\n",
        "\n",
        "df_cleanBuildings = df_cleanBuildings.astype({\"Zipcodes\": str})\n",
        "df_cleanUnits = df_cleanUnits.astype({\"Zipcodes\": str})\n",
        "df_cleanRentals = df_cleanRentals.astype({\"Zipcodes\": str}).drop(8)\n",
        "\n",
        "#!pip3 install -U geopandas fiona shapely pyproj geopy pysal descartes\n",
        "import geopandas as gpd\n",
        "#!mkdir data\n",
        "\n",
        "#!curl 'https://data-beta-nyc-files.s3.amazonaws.com/resources/6df127b1-6d04-4bb7-b983-07402a2c3f90/f4129d9aa6dd4281bc98d0f701629b76nyczipcodetabulationareas.geojson?Signature=RPeSQKj41E1yVg%2BDq%2BxQpyr75DU%3D&Expires=1619995115&AWSAccessKeyId=AKIAWM5UKMRH2KITC3QA' -o data/nyc-neighborhood.geojson\n",
        "\n",
        "\n",
        "df_nyc = gpd.GeoDataFrame.from_file('nyc-neighborhood.geojson.json')\n",
        "df_nyc = df_nyc.drop(df_nyc[df_nyc.borough != 'Manhattan'].index)\n",
        "df_nyc = df_nyc.drop(47)\n",
        "df_nyc.plot(linewidth=0.5, color='White',edgecolor = 'Black', figsize = (20,20))\n",
        "\n",
        "def geoJsonEditor(df_geoJson, df):\n",
        "  avgPriceDiffList2 = []\n",
        "\n",
        "  for index, row1 in df_geoJson.iterrows():\n",
        "    checker = False\n",
        "\n",
        "    for index, row2 in df.iterrows():\n",
        "      if row1['postalCode'] == row2['Zipcodes']:\n",
        "        avgPriceDiffList2.append(row2['Average Price Difference'])\n",
        "        checker = True\n",
        "    if checker == False:\n",
        "      avgPriceDiffList2.append('NA')\n",
        "\n",
        "  df_geoJson['Average Price Difference'] = avgPriceDiffList2\n",
        "  return df_geoJson\n",
        "\n",
        "df_nycBuildings = df_nyc.copy(deep = True) \n",
        "df_nycUnits = df_nyc.copy(deep = True) \n",
        "df_nycRentals = df_nyc.copy(deep = True) \n",
        "\n",
        "df_nycBuildings = geoJsonEditor(df_nycBuildings, df_cleanBuildings)\n",
        "df_nycUnits = geoJsonEditor(df_nycUnits, df_cleanUnits)\n",
        "df_nycRentals = geoJsonEditor(df_nycRentals, df_cleanRentals)\n",
        "\n",
        "nycGeoJsonList = [df_nycBuildings, df_nycUnits, df_nycRentals]\n",
        "nycDataframeList = [df_cleanBuildings, df_cleanUnits, df_cleanRentals]\n",
        "nycPredictionsList = [df_buildingsPrediction, df_unitsPredictions, df_rentalsPredictions]\n",
        "\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "def heatmapMaker(df):\n",
        "  import folium\n",
        "  from folium.plugins import HeatMap\n",
        "\n",
        "  lat = df['Latitude']\n",
        "  lng = df['Longitude']\n",
        "\n",
        "  map = folium.Map(\n",
        "      location=[40.85, -73.9712],\n",
        "      tiles='openstreetmap',\n",
        "      zoom_start=12\n",
        "  )\n",
        "  HeatMap(list(zip(lat, lng))).add_to(map)\n",
        "  return map\n",
        "\n",
        "heatmapBuildings = heatmapMaker(df_buildingsPrediction)\n",
        "heatmapUnits = heatmapMaker(df_unitsPredictions)\n",
        "heatmapRentals = heatmapMaker(df_rentalsPredictions)\n",
        "\n",
        "\n",
        "def choroplethMapMaker(df_geoJson, df):\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  m = folium.Map(location=[40.85, -73.9712], zoom_start=12)\n",
        "\n",
        "  choropleth = folium.Choropleth(\n",
        "      geo_data=df_geoJson,\n",
        "      name = 'choropleth',\n",
        "      data=df,\n",
        "      columns=[\"Zipcodes\", \"Average Price Difference\"],\n",
        "      key_on= 'feature.properties.postalCode',\n",
        "      fill_color=\"RdYlGn\",\n",
        "      fill_opacity=0.7,\n",
        "      line_opacity=1,\n",
        "      legend_name=\"Average Price Difference Between Predicted and Actual ($)\",\n",
        "  ).add_to(m)\n",
        "\n",
        "  folium.LayerControl().add_to(m)\n",
        "\n",
        "  style_function = lambda x: {'fillColor': '#ffffff', \n",
        "                              'color':'#000000', \n",
        "                              'fillOpacity': 0.1, \n",
        "                              'weight': 0.1}\n",
        "  highlight_function = lambda x: {'fillColor': '#000000', \n",
        "                                  'color':'#000000', \n",
        "                                  'fillOpacity': 0.50, \n",
        "                                  'weight': 0.1}\n",
        "  nyc = folium.features.GeoJson(\n",
        "      df_geoJson,\n",
        "      style_function=style_function, \n",
        "      control=False,\n",
        "      highlight_function=highlight_function, \n",
        "      tooltip=folium.features.GeoJsonTooltip(\n",
        "          fields=['Average Price Difference','postalCode'],\n",
        "          aliases=['Average Price Difference Between Predicted and Actual ($): ','Zipcode: '],\n",
        "          style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\") \n",
        "      )\n",
        "  )\n",
        "  m.add_child(nyc)\n",
        "  m.keep_in_front(nyc)\n",
        "  folium.LayerControl().add_to(m)\n",
        "  return m\n",
        "\n",
        "buildingChoropleth = choroplethMapMaker(df_nycBuildings, df_cleanBuildings)\n",
        "unitChoropleth = choroplethMapMaker(df_nycUnits, df_cleanUnits)\n",
        "rentalChoropleth = choroplethMapMaker(df_nycRentals, df_cleanRentals)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_C9T0e1uQ4X",
        "outputId": "4a6342d8-06f1-4891-b2c4-f162a281341d"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def connect_to_sql():\n",
        "  conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "      user='Divinus', \n",
        "      password='M1ZpoHfNx9U=', \n",
        "      host = 'jsedocc7.scrc.nyu.edu', \n",
        "      port=3306, \n",
        "      db='divinus',\n",
        "      encoding = 'utf-8'\n",
        "  )\n",
        "\n",
        "  engine = create_engine(conn_string)\n",
        "\n",
        "  return engine\n",
        "\n",
        "engine = connect_to_sql()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ABvNSSjtqFM",
        "outputId": "9e6e94a8-8278-44d8-a5e7-b219864d3e99"
      },
      "source": [
        "!gdown --id '1nB883hglmt8pAzKfUXCL5tzlAGkZ5vVP'\n",
        "rentals_model = xgb.XGBRegressor()\n",
        "rentals_model.load_model('/content/rentals_model.json')\n",
        "\n",
        "!gdown --id '1BJ0uP6PLPX1NZDqnVDcH0D8Z542jQN9-'\n",
        "units_model = xgb.XGBRegressor()\n",
        "units_model.load_model('/content/unit_sales_model.json')\n",
        "\n",
        "!gdown --id '1vjIbDUQT4DlA5A3Y_pgYvGCvW_zMG1tg'\n",
        "building_model = xgb.XGBRegressor()\n",
        "building_model.load_model('/content/buildings_model-3.json')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nB883hglmt8pAzKfUXCL5tzlAGkZ5vVP\n",
            "To: /content/rentals_model.json\n",
            "100% 1.81M/1.81M [00:00<00:00, 55.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BJ0uP6PLPX1NZDqnVDcH0D8Z542jQN9-\n",
            "To: /content/unit_sales_model.json\n",
            "2.44MB [00:00, 78.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vjIbDUQT4DlA5A3Y_pgYvGCvW_zMG1tg\n",
            "To: /content/buildings_model-3.json\n",
            "100% 55.7k/55.7k [00:00<00:00, 21.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWkyXe8CqsqJ",
        "outputId": "7f983b10-b05f-4bbb-e742-c44ab53c3762"
      },
      "source": [
        "%%writefile ml.py\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#connect to sql database\n",
        "\n",
        "def connect_to_sql():\n",
        "  conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "      user='Divinus', \n",
        "      password='M1ZpoHfNx9U=', \n",
        "      host = 'jsedocc7.scrc.nyu.edu', \n",
        "      port=3306, \n",
        "      db='divinus',\n",
        "      encoding = 'utf-8'\n",
        "  )\n",
        "\n",
        "  engine = create_engine(conn_string)\n",
        "\n",
        "  return engine\n",
        "\n",
        "engine = connect_to_sql()\n",
        "\n",
        "#create dummy variables using the category names, answers for the category, and all possible answers for each category\n",
        "\n",
        "def create_dummy_answers(variables, answers, cat_options):\n",
        "\n",
        "  all_dummies = []\n",
        "\n",
        "  for var in variables:\n",
        "    answer = answers[variables.index(var)]\n",
        "\n",
        "    options = list(cat_options[var].dropna())\n",
        "    options = [i for i in options if i]\n",
        "\n",
        "    dummies = []\n",
        "\n",
        "    if isinstance(answer, list) == True:\n",
        "      for item in options:\n",
        "        if item in answer:\n",
        "          dummies.append(1)\n",
        "        else:\n",
        "          dummies.append(0)\n",
        "    \n",
        "    else:\n",
        "      for item in options:\n",
        "        if answer == item:\n",
        "          dummies.append(1)\n",
        "        else:\n",
        "          dummies.append(0)\n",
        "\n",
        "    all_dummies += dummies\n",
        "\n",
        "  return all_dummies\n",
        "\n",
        "\n",
        "def rentals_predict(rental_regression_variables, r_cat_answers, rental_model):\n",
        "\n",
        "  rental_categorical_variables = ['Zipcode', 'Unit Type', 'Furnished', 'Building Type', '# Bedrooms', 'Rental Incentives', 'Amentities']\n",
        "\n",
        "  r_cat_options = pd.read_sql(\"SELECT * FROM rental_categorical_options\", con=engine)\n",
        "\n",
        "  r_cat_options.drop(r_cat_options.columns[r_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "  r_cat_dummy_answers = create_dummy_answers(rental_categorical_variables, r_cat_answers, r_cat_options)\n",
        "\n",
        "  all_answers = np.array(rental_regression_variables + r_cat_dummy_answers).reshape((1,-1))\n",
        "\n",
        "  rental_prediction = round(rental_model.predict(all_answers).item(), 2)\n",
        "\n",
        "  return rental_prediction\n",
        "\n",
        "\n",
        "def units_predict(unit_regression_variables, u_cat_answers, unit_model):\n",
        "\n",
        "  unit_categorical_variables = ['Zipcode', '# Bedrooms', 'Unit Type', 'Building Type', 'Amentities']\n",
        "\n",
        "  u_cat_options = pd.read_sql(\"SELECT * FROM unit_categorical_options\", con=engine)\n",
        "\n",
        "  u_cat_options.drop(u_cat_options.columns[u_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "  u_cat_dummy_answers = create_dummy_answers(unit_categorical_variables, u_cat_answers, u_cat_options)\n",
        "\n",
        "  all_answers = np.array(unit_regression_variables + u_cat_dummy_answers).reshape((1,-1))\n",
        "\n",
        "  unit_prediction = round(unit_model.predict(all_answers).item(), 2)\n",
        "\n",
        "  return unit_prediction\n",
        "\n",
        "#all of the rest is for building model \n",
        "#\n",
        "\n",
        "def fetch_bbl(street_num, street_name, apt_num): #apt_num value None if searching entire building, might be important for condos/apts\n",
        "    url = 'http://webapps.nyc.gov:8084/CICS/fin1/find001i'\n",
        "\n",
        "    borough = '1'\n",
        "\n",
        "    property_info = {\n",
        "        'FAPTNUM': apt_num,\n",
        "        'FBORO': borough,\n",
        "        'FFUNC': 'A',\n",
        "        'FHOUSENUM': street_num,\n",
        "        'FSTNAME': street_name\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, data=property_info)\n",
        "\n",
        "    soup = BeautifulSoup(response.text)\n",
        "\n",
        "    lot_tag = soup.find('input', attrs={'name': 'q49_lot'})\n",
        "    block_tag = soup.find('input', attrs={'name': 'q49_block_id'})\n",
        "\n",
        "    lot = None\n",
        "    block = None\n",
        "\n",
        "    if lot_tag and block_tag:\n",
        "        lot = lot_tag.get('value')\n",
        "        block = block_tag.get('value')\n",
        "\n",
        "    borough_block_lot = [borough, block, lot]\n",
        "\n",
        "    return borough_block_lot\n",
        "\n",
        "\n",
        "def fetch_building_data(bbl, sql_engine):\n",
        "  engine = sql_engine \n",
        "\n",
        "  borough = int(bbl[0])\n",
        "  block = int(bbl[1])\n",
        "  lot = int(bbl[2])\n",
        "\n",
        "  tax_match = pd.read_sql(\"SELECT * FROM tax_assessor WHERE BLOCK=%d AND LOT=%d ORDER BY YEAR DESC LIMIT 1\" % (block, lot), con=engine)\n",
        "\n",
        "  lot_frontage = tax_match['LOT_FRT'].item()\n",
        "  lot_depth = tax_match['LOT_DEP'].item()\n",
        "  building_front = tax_match['BLD_FRT'].item()\n",
        "  building_depth = tax_match['BLD_DEP'].item()\n",
        "  stories = tax_match['BLD_STORY'].item()\n",
        "  market_land_value = tax_match['CURMKTLAND'].item()\n",
        "  market_total_value = tax_match['CURMKTTOT'].item()\n",
        "  actual_total_value = tax_match['CURACTTOT'].item()\n",
        "  taxable_value = tax_match['CURTXBTOT'].item()\n",
        "  taxable_exemptions = tax_match['CURTXBEXTOT'].item()\n",
        "  tax_class = tax_match['CURTAXCLASS'].item()\n",
        "  building_class = tax_match['BLDG_CLASS'].item()\n",
        "  yr_built = tax_match.YRBUILT.item()\n",
        "\n",
        "  pluto_match = pd.read_sql(\"SELECT * FROM pluto WHERE block=%d AND lot=%d LIMIT 1\" % (block, lot), con=engine)\n",
        "\n",
        "  community_district = pluto_match.cd.item()\n",
        "  school_district = pluto_match.schooldist.item()\n",
        "  police_precinct = pluto_match.policeprct.item()\n",
        "  health_center = pluto_match.healthcenterdistrict.item()\n",
        "  health_area = pluto_match.healtharea.item()\n",
        "  lot_type = pluto_match.lottype.item()\n",
        "  latitude = pluto_match.latitude.item()\n",
        "  longitude = pluto_match.longitude.item()\n",
        "  zip = pluto_match.zipcode.item()\n",
        "  residential_units = pluto_match.unitsres.item()\n",
        "  commercial_units = pluto_match.unitstotal.item() - residential_units\n",
        "  land_sq_ft = pluto_match.lotarea.item()\n",
        "  building_sq_ft = pluto_match.bldgarea.item()\n",
        "\n",
        "  bbl_float = float(bbl[0]+bbl[1]+bbl[2])\n",
        "  bedbug_match = pd.read_sql(\"SELECT * FROM bedbug_reports WHERE BBL=%f AND 'Infested Dwelling Unit Count' > 0 AND 'Eradicated Unit Count' > 0\" % bbl_float, con=engine)\n",
        "  num_bedbug_reports = len(bedbug_match)\n",
        "\n",
        "  district = int(pluto_match.cd.item())\n",
        "  district_match = pd.read_sql(\"SELECT * FROM district_data WHERE borocd=%d\" % district, con=engine)\n",
        "\n",
        "  num_hospitals = district_match.count_hosp_clinic.item()\n",
        "  num_libraries = district_match.count_libraries.item()\n",
        "  num_parks = district_match.count_parks.item()\n",
        "  num_schools = district_match.count_public_schools.item()\n",
        "  crime_per_1000 = district_match.crime_per_1000.item()\n",
        "  rent_burden =  district_match.pct_hh_rent_burd.item()\n",
        "  mean_commute = district_match.mean_commute.item()\n",
        "  bach_degree = district_match.pct_bach_deg.item()\n",
        "  clean_streets = district_match.pct_clean_strts.item()\n",
        "  walking_distance_parks = district_match.pct_served_parks.item()\n",
        "  unemployment = district_match.unemployment.item()\n",
        "\n",
        "  housing_index_match = pd.read_sql(\"SELECT * FROM ny_housing_index ORDER BY DATE DESC LIMIT 1\", con=engine)\n",
        "  housing_index = housing_index_match.NYXRSA.item()\n",
        "\n",
        "  cpi_match = pd.read_sql(\"SELECT * FROM cpi ORDER BY DATE DESC LIMIT 1\", con=engine)\n",
        "  cpi = cpi_match.CPIAUCSL.item()\n",
        "\n",
        "  categorical_variables = [None, building_class, zip, tax_class, school_district, health_center, lot_type, community_district]\n",
        "\n",
        "  regression_variables = [residential_units, commercial_units, land_sq_ft, building_sq_ft, yr_built, lot_frontage, lot_depth,\n",
        "                        building_front, building_depth, stories, market_land_value, market_total_value, actual_total_value,\n",
        "                        taxable_value, taxable_exemptions, num_hospitals, num_libraries, num_parks, num_schools, crime_per_1000, rent_burden,\n",
        "                        mean_commute, bach_degree, clean_streets, walking_distance_parks, unemployment, num_bedbug_reports, None, housing_index, cpi]\n",
        "\n",
        "  return regression_variables, categorical_variables\n",
        "\n",
        "def buildings_predict(bbl, sql_engine, building_model):\n",
        "  building_data = fetch_building_data(bbl, sql_engine)\n",
        "  building_regression_variables = building_data[0]\n",
        "  b_cat_answers = building_data[1]\n",
        "\n",
        "  building_categorical_variables = ['Neighborhood', 'Building Class', 'Zip', 'Tax Class', 'School District', 'Health Center District', 'Lot Type', 'Community District']\n",
        "\n",
        "  b_cat_options = pd.read_sql(\"SELECT * FROM building_categorical_options\", con=engine)\n",
        "  b_cat_options.drop(b_cat_options.columns[b_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "  b_cat_dummy_answers = create_dummy_answers(building_categorical_variables, b_cat_answers, b_cat_options)\n",
        "\n",
        "  all_answers = np.array(building_regression_variables + b_cat_dummy_answers).reshape((1,-1))\n",
        "\n",
        "  building_prediction = round(building_model.predict(all_answers).item(), 0)\n",
        "\n",
        "  return building_prediction"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ml.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSzn6hWo0TCH"
      },
      "source": [
        "#rental cat variables\n",
        "rental_categorical_variables = ['Zipcode', 'Unit Type', 'Furnished', 'Building Type', '# Bedrooms', 'Rental Incentives', 'Amentities']\n",
        "r_cat_options = pd.read_sql(\"SELECT * FROM rental_categorical_options\", con=engine)\n",
        "r_cat_options.drop(r_cat_options.columns[r_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "zip_codes = tuple(r_cat_options['Zipcode'].dropna())\n",
        "unit_types = tuple(r_cat_options['Unit Type'].dropna())\n",
        "furnished = tuple(r_cat_options['Furnished'].dropna())\n",
        "building_types = tuple(r_cat_options['Building Type'].dropna())\n",
        "num_bedrooms = tuple(r_cat_options['# Bedrooms'].dropna())\n",
        "rental_incentives = tuple(r_cat_options['Rental Incentives'].dropna())\n",
        "amentites = tuple(r_cat_options['Amentities'].dropna())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWBpyx_e5TFh"
      },
      "source": [
        "#unit cat variables\n",
        "u_cat_options = pd.read_sql(\"SELECT * FROM unit_categorical_options\", con=engine)\n",
        "u_cat_options.drop(u_cat_options.columns[u_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "#unit_zip = st.selectbox('Zipcode: ', tuple(u_cat_options['Zipcode'].dropna()))\n",
        "#unit_bedrooms = st.selectbox('Number of Bedrooms: ', tuple(u_cat_options['# Bedrooms'].dropna()))\n",
        "#unit_unit_type = st.selectbox('Unit Type: ', tuple(u_cat_options['Unit Type'].dropna()))\n",
        "#unit_unit_building_type = st.selectbox('Building Type: ', tuple(u_cat_options['Building Type'].dropna()))\n",
        "#unit_unit_amenities = st.selectbox('Amenities: ', tuple(u_cat_options['Amentities'].dropna()))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsvDGkae7s47",
        "outputId": "5766161f-d019-444b-a2c1-7b3afc98f809"
      },
      "source": [
        "!pip install rake_nltk\n",
        "!sudo apt-get install python3-dev libmysqlclient-dev > /dev/null\n",
        "!pip install mysqlclient > /dev/null\n",
        "!sudo pip3 install -U sql_magic > /dev/null\n",
        "!pip install psycopg2-binary > /dev/null"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake_nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake_nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVBEaTVY8Weq",
        "outputId": "90e2a2c3-b17e-432b-fd2f-0cd8e2ba9e06"
      },
      "source": [
        "%%writefile nlp.py\n",
        "\n",
        "def listingDescriptionAdvisorRentals(nlp_input,print):\n",
        "  import requests\n",
        "  import nltk\n",
        "  from nltk import word_tokenize\n",
        "  from nltk.util import ngrams\n",
        "  from collections import Counter\n",
        "  from rake_nltk import Metric, Rake\n",
        "  import pandas as pd\n",
        "  import operator\n",
        "  from scipy import stats\n",
        "  nltk.download('punkt', quiet=True)\n",
        "  nltk.download('wordnet', quiet=True)\n",
        "  from sqlalchemy import create_engine\n",
        "  import random\n",
        "  from nltk.stem.wordnet import WordNetLemmatizer\n",
        "  from nltk.corpus import stopwords \n",
        "  import time\n",
        "  import re\n",
        "  \n",
        "\n",
        "  if len(nlp_input.replace('.', '').split()) >= 3:\n",
        "    conn_string_realEstate = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "        user='Divinus', \n",
        "        password='M1ZpoHfNx9U=', \n",
        "        host = 'jsedocc7.scrc.nyu.edu', \n",
        "        port=3306, \n",
        "        db='divinus',\n",
        "        encoding = 'utf-8'\n",
        "    )\n",
        "    engine_buildings = create_engine(conn_string_realEstate)\n",
        "\n",
        "    query = '''SELECT * FROM rentalsRAKE'''\n",
        "    queryUni = '''SELECT * FROM rentalsUnigrams'''\n",
        "    queryBi = '''SELECT * FROM rentalsBigrams'''\n",
        "    queryTri = '''SELECT * FROM rentalsTrigrams'''\n",
        "\n",
        "    df_rentalRake = pd.read_sql(query, con=engine_buildings)\n",
        "    df_unigram = pd.read_sql(queryUni, con=engine_buildings)\n",
        "    df_bigram = pd.read_sql(queryBi, con=engine_buildings)\n",
        "    df_trigram = pd.read_sql(queryTri, con=engine_buildings)\n",
        "\n",
        "    nlp_input = nlp_input.strip('.')\n",
        "\n",
        "    url = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze\"\n",
        "    username = \"apikey\"\n",
        "    password = \"PyUxQEDu8LrmOwB4nHcuHVan2QMMfj7RIlz21idJPU5v\"\n",
        "\n",
        "    parameters = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': nlp_input,\n",
        "        'language' : 'en',\n",
        "    }\n",
        "    r = requests.get(url, params=parameters, auth=(username, password))\n",
        "    sentimentScore = r.json()['sentiment']['document']['score']\n",
        "    \n",
        "    if sentimentScore > 0.8574199299109228:\n",
        "      print('Your rental listing description\\'s sentiment value looks great! Your current description has a sentiment score of {score}, which, on average, suggests that your house will sell for more than our predicted price!'.format(score = sentimentScore))\n",
        "    else:\n",
        "      print('Analyzing your rental listing description...\\n')\n",
        "      time.sleep(2)\n",
        "      sentences = nlp_input.split('.')\n",
        "      sentimentScoreList = [] \n",
        "      for sentence in sentences:\n",
        "        parametersTemp = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': sentence,\n",
        "        'language' : 'en',\n",
        "        }\n",
        "        rTemp = requests.get(url, params=parametersTemp, auth=(username, password))\n",
        "        sentimentScoreList.append(rTemp.json()['sentiment']['document']['score'])\n",
        "\n",
        "      lowestSentimentScore = 1.1\n",
        "      sentenceNumber = 0\n",
        "      \n",
        "      for x in range(len(sentimentScoreList)):\n",
        "        if sentimentScoreList[x] < lowestSentimentScore:\n",
        "          lowestSentimentScore = sentimentScoreList[x]\n",
        "          sentenceNumber = x\n",
        "\n",
        "      worstSentence = sentences[sentenceNumber]\n",
        "      sentenceNumber += 1\n",
        "\n",
        "      worstSentenceTokenized = nltk.word_tokenize(worstSentence)\n",
        "      wordSentimentList = []\n",
        "\n",
        "      for word in worstSentenceTokenized:\n",
        "        parametersTemp = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': word,\n",
        "        'language' : 'en',\n",
        "        }\n",
        "        rTemp = requests.get(url, params=parametersTemp, auth=(username, password))\n",
        "        wordSentimentList.append(rTemp.json()['sentiment']['document']['score'])\n",
        "\n",
        "      lowestSentimentWord = wordSentimentList[0]\n",
        "      wordNumber = 0\n",
        "\n",
        "      for x in range(1, len(wordSentimentList)):\n",
        "        if wordSentimentList[x] < lowestSentimentWord:\n",
        "          lowestSentimentWord = wordSentimentList[x]\n",
        "          wordNumber = x\n",
        "\n",
        "      worstWord = worstSentenceTokenized[wordNumber]\n",
        "\n",
        "      print('Your current rental listing description has an overall sentiment score of: {score}, which is {diff} lower than what we would recomend to improve your chances of selling your property above the predicted price.\\nIn your current listing, sentence {sentence} had the lowest sentiment score of {sentScore}, while the worst word in this sentence was \"{word}\" with a sentiment score of {scoreWord}, which we would recomend you change for a more positive alternative.\\n'.format(score = sentimentScore, diff = round(.85 - sentimentScore, 2), sentence = sentenceNumber, sentScore = lowestSentimentScore, word = worstWord, scoreWord = lowestSentimentWord))\n",
        "      \n",
        "      \n",
        "    print('Please wait while we further analyze your rental listing description...\\n')\n",
        "    time.sleep(2)\n",
        "    ## RAKE\n",
        "\n",
        "    r1 = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
        "    r2 = Rake(ranking_metric=Metric.WORD_DEGREE)\n",
        "    r1.extract_keywords_from_text(nlp_input)\n",
        "    r2.extract_keywords_from_text(nlp_input)\n",
        "\n",
        "    wordFreqList = []\n",
        "    phraseList1 = []\n",
        "    wordDegreeList = []\n",
        "    phraseList2 = []\n",
        "    ratioList = []\n",
        "\n",
        "    for x in r1.get_ranked_phrases_with_scores():\n",
        "      wordFreqList.append(x[0])\n",
        "      phraseList1.append(x[1])\n",
        "    for x in r2.get_ranked_phrases_with_scores():\n",
        "      wordDegreeList.append(x[0])\n",
        "      phraseList2.append(x[1])\n",
        "\n",
        "    data_rake1 = {'Word Frequency' : wordFreqList, 'Phrase' : phraseList1}\n",
        "    df_rake1 = pd.DataFrame(data_rake1)\n",
        "    data_rake2 = {'Word Degree' : wordDegreeList, 'Phrase' : phraseList2}\n",
        "    df_rake2 = pd.DataFrame(data_rake2)\n",
        "    df_rake = pd.merge(df_rake1, df_rake2, on='Phrase')\n",
        "\n",
        "    highestRankedIndex = 999999999999999999999999\n",
        "    keyword = ''\n",
        "    wordRatio = 0\n",
        "\n",
        "    df_rentalRake.sort_values(by='Word Ratio', ascending=True)\n",
        "\n",
        "    if df_rake.Phrase.isin(df_rentalRake.Phrase).any():\n",
        "      for phrase in range(len(df_rake['Phrase'])):\n",
        "        if len(df_rentalRake.loc[df_rentalRake.isin([df_rake['Phrase'][phrase]]).any(axis=1)].index) != 0:\n",
        "          tempIndex = df_rentalRake.loc[df_rentalRake.isin([df_rake['Phrase'][phrase]]).any(axis=1)].index[0]\n",
        "          if tempIndex < highestRankedIndex:\n",
        "            highestRankedIndex = tempIndex\n",
        "            keyword = df_rentalRake['Phrase'][tempIndex]\n",
        "            wordRatio = df_rentalRake['Word Ratio'][tempIndex]\n",
        "      percentile = round(100 - stats.percentileofscore(df_rentalRake['Word Ratio'].index, highestRankedIndex),2)\n",
        "    else:\n",
        "      percentile = 'N/A'\n",
        "\n",
        "    if percentile != 'N/A':\n",
        "      suggestion1 = df_rentalRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      suggestion2 = df_rentalRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      suggestion3 = df_rentalRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      print('According to our historical rental data, your current rental listing descriptions highest ranked keyword or phrase is: \"{keyword}\", which has a frequency to degree ratio of {ratio}. This keyword/phrase\\'s ratio places it in the {percentile} percentile of all our keywords and phrases. We would recomend you add one of the following to your listing description:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(keyword = keyword, ratio = wordRatio, percentile = percentile, sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    else:\n",
        "      suggestion1 = df_rentalRake['Phrase'][random.randrange(0, len(df_rentalRake))]\n",
        "      suggestion2 = df_rentalRake['Phrase'][random.randrange(0, len(df_rentalRake))]\n",
        "      suggestion3 = df_rentalRake['Phrase'][random.randrange(0, len(df_rentalRake))]\n",
        "      print('According to our historical rental data, your current rental listing descriptions does not have any relevant keyword of phrase. Because your listing does not currently have any keywords/phrase, we would recomend you add one of the following to your listing description:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "\n",
        "    print('\\nPlease wait while we further analyze your rental listing description...\\n')\n",
        "    time.sleep(2)\n",
        "    ### N-grams\n",
        "\n",
        "    corpus = []\n",
        "\n",
        "    nlp_input = re.sub('[^a-zA-Z0-9]', ' ', nlp_input)\n",
        "    nlp_input = nlp_input.lower()\n",
        "    nlp_input = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", nlp_input)\n",
        "    nlp_input = re.sub(\"(\\\\d|\\\\W)+\",\" \",nlp_input)\n",
        "    nlp_input = nlp_input.split()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemma = WordNetLemmatizer()\n",
        "    y = [lemma.lemmatize(word) for word in nlp_input if not word in stop_words]\n",
        "    corpus.append(y)\n",
        "    \n",
        "    corpusStr = ''\n",
        "\n",
        "    for list1 in corpus:\n",
        "      for word in list1:\n",
        "        corpusStr += word + ' '\n",
        "\n",
        "    tokens = nltk.word_tokenize(corpusStr)\n",
        "\n",
        "    uni = ngrams(tokens, 1)\n",
        "    bi = ngrams(tokens, 2)\n",
        "    tri = ngrams(tokens, 3)\n",
        "    uniWordList = []\n",
        "    biWordList = []\n",
        "    triWordList = []\n",
        "\n",
        "    for word in uni:\n",
        "      if word[0] not in uniWordList and df_unigram.isin([word[0]]).any().any():\n",
        "        uniWordList.append(word[0])\n",
        "    for word in bi:\n",
        "      if word[0] + ' ' + word[1] not in biWordList and df_bigram.isin([word[0] + ' ' + word[1]]).any().any():\n",
        "        biWordList.append(word[0] + ' ' + word[1])\n",
        "    for word in tri:\n",
        "      if word[0] + ' ' + word[1] + ' ' + word[2] not in triWordList and df_trigram.isin([word[0] + ' ' + word[1] + ' '+ word[2]]).any().any():\n",
        "        triWordList.append(word[0] + ' ' + word[1] + ' '+ word[2])\n",
        "\n",
        "    if len(uniWordList) != 0:\n",
        "      print('Your current rental listing description has {number} of our 20 most common words used in rental descriptions. These {number} word(s) are: {words}'.format(number = len(uniWordList,), words = uniWordList))\n",
        "    else:\n",
        "      suggestion1 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      suggestion2 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      suggestion3 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      print('Your current listing description has 0 of our 20 most common words used in rental descriptions. If you would like to include some of the most popular words in your description, here are some ideas:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    if len(biWordList) != 0:\n",
        "      print('\\nYour current rental listing description has {number} of our 20 most common two word phrases used in rental descriptions. These {number} phrase(s) are: {words}'.format(number = len(biWordList,), words = biWordList))\n",
        "    else:\n",
        "      suggestion1 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      suggestion2 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      suggestion3 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      print('\\nYour current rental listing description has 0 of our 20 most common two word phrases used in rental descriptions. If you would like to include some of the most popular two word phrases in your description, here are some ideas:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    if len(triWordList) != 0:\n",
        "      print('\\nYour current rental listing description has {number} of our 20 most common three word phrases used in rental descriptions. These {number} word(s) are: {words}'.format(number = len(triWordList,), words = triWordList))\n",
        "    else:\n",
        "      suggestion1 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      suggestion2 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      suggestion3 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      print('\\nYour current rental listing description has 0 of our 20 most common three word phrases used in rental descriptions. If you would like to include some of the most popular three word phrases in your description, here are some ideas:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "\n",
        "    print('\\n\\nThat\\'s all we have for your rental listing description! Once you edit your description or need new word/phrase suggestions, please enter your revised listing or pick from the word cloud below for the top 50 words and phrases from our historical data...\\nThanks!')\n",
        "  else:\n",
        "    print('Sorry please enter a description that has atleast 3 words!')\n",
        "  \n",
        "  return ''\n",
        "  \n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def listingDescriptionAdvisorUnits(nlp_input, print):\n",
        "  import requests\n",
        "  import nltk\n",
        "  from nltk import word_tokenize\n",
        "  from nltk.util import ngrams\n",
        "  from collections import Counter\n",
        "  from rake_nltk import Metric, Rake\n",
        "  import pandas as pd\n",
        "  import operator\n",
        "  from scipy import stats\n",
        "  nltk.download('punkt', quiet=True)\n",
        "  nltk.download('wordnet', quiet=True)\n",
        "  from sqlalchemy import create_engine\n",
        "  import random\n",
        "  from nltk.stem.wordnet import WordNetLemmatizer\n",
        "  from nltk.corpus import stopwords \n",
        "  import time\n",
        "  import re\n",
        "\n",
        "  if len(nlp_input.replace('.', '').split()) >= 3:\n",
        "    conn_string_realEstate = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "        user='Divinus', \n",
        "        password='M1ZpoHfNx9U=', \n",
        "        host = 'jsedocc7.scrc.nyu.edu', \n",
        "        port=3306, \n",
        "        db='divinus',\n",
        "        encoding = 'utf-8'\n",
        "    )\n",
        "    engine_buildings = create_engine(conn_string_realEstate)\n",
        "\n",
        "    query = '''SELECT * FROM unitsRAKE'''\n",
        "    queryUni = '''SELECT * FROM unitsUnigrams'''\n",
        "    queryBi = '''SELECT * FROM unitsBigrams'''\n",
        "    queryTri = '''SELECT * FROM unitsTrigrams'''\n",
        "\n",
        "    df_unitsRake = pd.read_sql(query, con=engine_buildings)\n",
        "    df_unigram = pd.read_sql(queryUni, con=engine_buildings)\n",
        "    df_bigram = pd.read_sql(queryBi, con=engine_buildings)\n",
        "    df_trigram = pd.read_sql(queryTri, con=engine_buildings)\n",
        "\n",
        "    nlp_input = nlp_input.strip('.')\n",
        "\n",
        "    url = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze\"\n",
        "    username = \"apikey\"\n",
        "    password = \"PyUxQEDu8LrmOwB4nHcuHVan2QMMfj7RIlz21idJPU5v\"\n",
        "\n",
        "    parameters = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': nlp_input,\n",
        "        'language' : 'en',\n",
        "    }\n",
        "    r = requests.get(url, params=parameters, auth=(username, password))\n",
        "    sentimentScore = r.json()['sentiment']['document']['score']\n",
        "    \n",
        "    if sentimentScore > 0.8574199285044533:\n",
        "      print('Your unit listing description\\'s sentiment value looks great! Your current description has a sentiment score of {score}, which, on average, suggests that your house will sell for more than our predicted price!'.format(score = sentimentScore))\n",
        "    else:\n",
        "      print('Analyzing your unit listing description...\\n')\n",
        "      time.sleep(2)\n",
        "      sentences = nlp_input.split('.')\n",
        "      sentimentScoreList = [] \n",
        "      for sentence in sentences:\n",
        "        parametersTemp = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': sentence,\n",
        "        'language' : 'en',\n",
        "        }\n",
        "        rTemp = requests.get(url, params=parametersTemp, auth=(username, password))\n",
        "        sentimentScoreList.append(rTemp.json()['sentiment']['document']['score'])\n",
        "\n",
        "      lowestSentimentScore = 1.1\n",
        "      sentenceNumber = 0\n",
        "      \n",
        "      for x in range(len(sentimentScoreList)):\n",
        "        if sentimentScoreList[x] < lowestSentimentScore:\n",
        "          lowestSentimentScore = sentimentScoreList[x]\n",
        "          sentenceNumber = x\n",
        "\n",
        "      worstSentence = sentences[sentenceNumber]\n",
        "      sentenceNumber += 1\n",
        "\n",
        "      worstSentenceTokenized = nltk.word_tokenize(worstSentence)\n",
        "      wordSentimentList = []\n",
        "\n",
        "      for word in worstSentenceTokenized:\n",
        "        parametersTemp = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2018-11-16',\n",
        "        'text': word,\n",
        "        'language' : 'en',\n",
        "        }\n",
        "        rTemp = requests.get(url, params=parametersTemp, auth=(username, password))\n",
        "        wordSentimentList.append(rTemp.json()['sentiment']['document']['score'])\n",
        "\n",
        "      lowestSentimentWord = wordSentimentList[0]\n",
        "      wordNumber = 0\n",
        "\n",
        "      for x in range(1, len(wordSentimentList)):\n",
        "        if wordSentimentList[x] < lowestSentimentWord:\n",
        "          lowestSentimentWord = wordSentimentList[x]\n",
        "          wordNumber = x\n",
        "\n",
        "      worstWord = worstSentenceTokenized[wordNumber]\n",
        "\n",
        "      print('Your current unit listing description has an overall sentiment score of: {score}, which is {diff} lower than what we would recomend to improve your chances of selling your property above the predicted price.\\nIn your current listing, sentence {sentence} had the lowest sentiment score of {sentScore}, while the worst word in this sentence was \"{word}\" with a sentiment score of {scoreWord}, which we would recomend you change for a more positive alternative.\\n'.format(score = sentimentScore, diff = round(.85 - sentimentScore, 2), sentence = sentenceNumber, sentScore = lowestSentimentScore, word = worstWord, scoreWord = lowestSentimentWord))\n",
        "      \n",
        "      \n",
        "    print('Please wait while we further analyze your unit listing description...\\n')\n",
        "    time.sleep(2)\n",
        "    ## RAKE\n",
        "\n",
        "    r1 = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
        "    r2 = Rake(ranking_metric=Metric.WORD_DEGREE)\n",
        "    r1.extract_keywords_from_text(nlp_input)\n",
        "    r2.extract_keywords_from_text(nlp_input)\n",
        "\n",
        "    wordFreqList = []\n",
        "    phraseList1 = []\n",
        "    wordDegreeList = []\n",
        "    phraseList2 = []\n",
        "    ratioList = []\n",
        "\n",
        "    for x in r1.get_ranked_phrases_with_scores():\n",
        "      wordFreqList.append(x[0])\n",
        "      phraseList1.append(x[1])\n",
        "    for x in r2.get_ranked_phrases_with_scores():\n",
        "      wordDegreeList.append(x[0])\n",
        "      phraseList2.append(x[1])\n",
        "\n",
        "    data_rake1 = {'Word Frequency' : wordFreqList, 'Phrase' : phraseList1}\n",
        "    df_rake1 = pd.DataFrame(data_rake1)\n",
        "    data_rake2 = {'Word Degree' : wordDegreeList, 'Phrase' : phraseList2}\n",
        "    df_rake2 = pd.DataFrame(data_rake2)\n",
        "    df_rake = pd.merge(df_rake1, df_rake2, on='Phrase')\n",
        "\n",
        "    highestRankedIndex = 999999999999999999999999\n",
        "    keyword = ''\n",
        "    wordRatio = 0\n",
        "\n",
        "    df_unitsRake.sort_values(by='Word Ratio', ascending=True)\n",
        "\n",
        "    if df_rake.Phrase.isin(df_unitsRake.Phrase).any():\n",
        "      for phrase in range(len(df_rake['Phrase'])):\n",
        "        if len(df_unitsRake.loc[df_unitsRake.isin([df_rake['Phrase'][phrase]]).any(axis=1)].index) != 0:\n",
        "          tempIndex = df_unitsRake.loc[df_unitsRake.isin([df_rake['Phrase'][phrase]]).any(axis=1)].index[0]\n",
        "          if tempIndex < highestRankedIndex:\n",
        "            highestRankedIndex = tempIndex\n",
        "            keyword = df_unitsRake['Phrase'][tempIndex]\n",
        "            wordRatio = df_unitsRake['Word Ratio'][tempIndex]\n",
        "      percentile = round(100 - stats.percentileofscore(df_unitsRake['Word Ratio'].index, highestRankedIndex),2)\n",
        "    else:\n",
        "      percentile = 'N/A'\n",
        "\n",
        "    if percentile != 'N/A':\n",
        "      suggestion1 = df_unitsRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      suggestion2 = df_unitsRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      suggestion3 = df_unitsRake['Phrase'][random.randrange(0, highestRankedIndex)]\n",
        "      print('According to our historical unit data, your current listing descriptions highest ranked keyword or phrase is: \"{keyword}\", which has a frequency to degree ratio of {ratio}. This keyword/phrase\\'s ratio places it in the {percentile} percentile of all our keywords and phrases. We would recomend you add one of the following to your listing description:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(keyword = keyword, ratio = wordRatio, percentile = percentile, sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    else:\n",
        "      suggestion1 = df_unitsRake['Phrase'][random.randrange(0, len(df_unitsRake))]\n",
        "      suggestion2 = df_unitsRake['Phrase'][random.randrange(0, len(df_unitsRake))]\n",
        "      suggestion3 = df_unitsRake['Phrase'][random.randrange(0, len(df_unitsRake))]\n",
        "      print('According to our historical unit data, your current listing descriptions does not have any relevant keyword of phrase. Because your listing does not currently have any keywords/phrase, we would recomend you add one of the following to your listing description: \\n\\nOption 1: {sug1}\\nOption 2: {sug2}\\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "\n",
        "    print('\\nPlease wait while we further analyze your unit listing description...\\n')\n",
        "    time.sleep(2)\n",
        "    ### N-grams\n",
        "\n",
        "    corpus = []\n",
        "\n",
        "    nlp_input = re.sub('[^a-zA-Z0-9]', ' ', nlp_input)\n",
        "    nlp_input = nlp_input.lower()\n",
        "    nlp_input = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", nlp_input)\n",
        "    nlp_input = re.sub(\"(\\\\d|\\\\W)+\",\" \",nlp_input)\n",
        "    nlp_input = nlp_input.split()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemma = WordNetLemmatizer()\n",
        "    y = [lemma.lemmatize(word) for word in nlp_input if not word in stop_words]\n",
        "    corpus.append(y)\n",
        "    \n",
        "    corpusStr = ''\n",
        "\n",
        "    for list1 in corpus:\n",
        "      for word in list1:\n",
        "        corpusStr += word + ' '\n",
        "\n",
        "    tokens = nltk.word_tokenize(corpusStr)\n",
        "\n",
        "    uni = ngrams(tokens, 1)\n",
        "    bi = ngrams(tokens, 2)\n",
        "    tri = ngrams(tokens, 3)\n",
        "    uniWordList = []\n",
        "    biWordList = []\n",
        "    triWordList = []\n",
        "\n",
        "    for word in uni:\n",
        "      if word[0] not in uniWordList and df_unigram.isin([word[0]]).any().any():\n",
        "        uniWordList.append(word[0])\n",
        "    for word in bi:\n",
        "      if word[0] + ' ' + word[1] not in biWordList and df_bigram.isin([word[0] + ' ' + word[1]]).any().any():\n",
        "        biWordList.append(word[0] + ' ' + word[1])\n",
        "    for word in tri:\n",
        "      if word[0] + ' ' + word[1] + ' ' + word[2] not in triWordList and df_trigram.isin([word[0] + ' ' + word[1] + ' '+ word[2]]).any().any():\n",
        "        triWordList.append(word[0] + ' ' + word[1] + ' '+ word[2])\n",
        "\n",
        "    if len(uniWordList) != 0:\n",
        "      print('Your current unit listing description has {number} of our 20 most common words used in unit descriptions. These {number} word(s) are: {words}'.format(number = len(uniWordList,), words = uniWordList))\n",
        "    else:\n",
        "      suggestion1 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      suggestion2 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      suggestion3 = df_unigram['Unigram'][random.randrange(0, len(df_unigram))]\n",
        "      print('Your current unit listing description has 0 of our 20 most common words used in unit descriptions. If you would like to include some of the most popular words in your description, here are some ideas:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    if len(biWordList) != 0:\n",
        "      print('\\nYour current unit listing description has {number} of our 20 most common two word phrases used in unit descriptions. These {number} phrase(s) are: {words}'.format(number = len(biWordList,), words = biWordList))\n",
        "    else:\n",
        "      suggestion1 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      suggestion2 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      suggestion3 = df_bigram['Bigrams'][random.randrange(0, len(df_bigram))]\n",
        "      print('\\nYour current unit listing description has 0 of our 20 most common two word phrases used in unit descriptions. If you would like to include some of the most popular two word phrases in your description, here are some ideas:  \\nOption 1: {sug1}  \\nOption 2: {sug2}  \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    if len(triWordList) != 0:\n",
        "      print('\\nYour current unit listing description has {number} of our 20 most common three word phrases used in unit descriptions. These {number} word(s) are: {words}'.format(number = len(triWordList,), words = triWordList))\n",
        "    else:\n",
        "      suggestion1 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      suggestion2 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      suggestion3 = df_trigram['Trigrams'][random.randrange(0, len(df_trigram))]\n",
        "      print('\\nYour current unit listing description has 0 of our 20 most common three word phrases used in unit descriptions. If you would like to include some of the most popular three word phrases in your description, here are some ideas:  \\nOption 1: {sug1} \\nOption 2: {sug2} \\nOption 3: {sug3}'.format(sug1 = suggestion1, sug2 = suggestion2, sug3 = suggestion3))\n",
        "    \n",
        "    print('\\n\\nThat\\'s all we have for your unit listing description! Once you edit your description or need new word/phrase suggestions, please enter your revised listing or pick from the word cloud below for the top 50 words and phrases from our historical data...\\nThanks!')\n",
        "  else:\n",
        "    print('Sorry please enter a description that has atleast 3 words!')\n",
        "  return ''"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting nlp.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHRyprPpCHkt",
        "outputId": "d10ca78b-c7ed-4434-f89a-27448ef3174a"
      },
      "source": [
        "%%writefile graph.py\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "conn_string_realEstate = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "    user='Divinus', \n",
        "    password='M1ZpoHfNx9U=', \n",
        "    host = 'jsedocc7.scrc.nyu.edu', \n",
        "    port=3306, \n",
        "    db='divinus',\n",
        "    encoding = 'utf-8'\n",
        ")\n",
        "engine_buildings = create_engine(conn_string_realEstate)\n",
        "\n",
        "query = '''\n",
        "SELECT * FROM unitsRAKE\n",
        "'''\n",
        "\n",
        "query2 = '''\n",
        "SELECT * FROM rentalsRAKE\n",
        "'''\n",
        "\n",
        "df_rake = pd.read_sql(query, con=engine_buildings)\n",
        "df_rakeRentals = pd.read_sql(query2, con=engine_buildings)\n",
        "\n",
        "fig = px.scatter(df_rake, x=\"Word Frequency\", y=\"Word Degree\", log_x=False,\n",
        "                 hover_name=\"Phrase\", hover_data=[\"Word Ratio\"])\n",
        "figRentals = px.scatter(df_rakeRentals, x=\"Word Frequency\", y=\"Word Degree\", log_x=False,\n",
        "                 hover_name=\"Phrase\", hover_data=[\"Word Ratio\"])\n",
        "fig.show()\n",
        "figRentals.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing graph.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_v_4SZk0TGf",
        "outputId": "5c9770ef-c9af-4019-c520-7cbd763f32d0"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st, pandas as pd\n",
        "from test import *\n",
        "from streamlit_folium import folium_static\n",
        "from ml import *\n",
        "from nlp import *\n",
        "from graph import *\n",
        "\n",
        "rentals_model = xgb.XGBRegressor()\n",
        "rentals_model.load_model('/content/rentals_model.json')\n",
        "\n",
        "units_model = xgb.XGBRegressor()\n",
        "units_model.load_model('/content/unit_sales_model.json')\n",
        "\n",
        "building_model = xgb.XGBRegressor()\n",
        "building_model.load_model('/content/buildings_model-3.json')\n",
        "\n",
        "u_cat_options = pd.read_sql(\"SELECT * FROM unit_categorical_options\", con=engine)\n",
        "u_cat_options.drop(u_cat_options.columns[u_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "rental_categorical_variables = ['Zipcode', 'Unit Type', 'Furnished', 'Building Type', '# Bedrooms', 'Rental Incentives', 'Amentities']\n",
        "r_cat_options = pd.read_sql(\"SELECT * FROM rental_categorical_options\", con=engine)\n",
        "r_cat_options.drop(r_cat_options.columns[r_cat_options.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "zip_codes = tuple(r_cat_options['Zipcode'].dropna())\n",
        "unit_types = tuple(r_cat_options['Unit Type'].dropna())\n",
        "\n",
        "buildingChoropleth = choroplethMapMaker(df_nycBuildings, df_cleanBuildings)\n",
        "unitChoropleth = choroplethMapMaker(df_nycUnits, df_cleanUnits)\n",
        "rentalChoropleth = choroplethMapMaker(df_nycRentals, df_cleanRentals)\n",
        "heatmapBuildings = heatmapMaker(df_buildingsPrediction)\n",
        "heatmapUnits = heatmapMaker(df_unitsPredictions)\n",
        "heatmapRentals = heatmapMaker(df_rentalsPredictions)\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"Divinus\",\"page_icon\":\":house:\",\"layout\":\"wide\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "def main():\n",
        "  st.title(\"Divinus Real Estate Price Appraisal\")\n",
        "  st.subheader(\"Projects in Programming, Sedoc Spring 2021\")\n",
        "  menu = [\"About\", \"Data\", 'Unit Price Predictor','Rental Price Predictor','Building Price Predictor', 'Unit Listing Description Analysis', 'Rental Listing Description Analysis']\n",
        "  choice = st.sidebar.selectbox('Menu',menu)\n",
        "\n",
        "\n",
        "\n",
        "  if choice == 'Unit Price Predictor':\n",
        "    folium_static(heatmapUnits)\n",
        "    folium_static(unitChoropleth)\n",
        "    st.write('Input values to generate predicted price:\\n')\n",
        "\n",
        "    #regression variables\n",
        "    num_bathrooms = st.text_input('Number of Bathrooms:')\n",
        "    sq_ft = st.text_input('Square Footage:')\n",
        "    days_on_market = st.text_input('Days on Market:')\n",
        "    annual_tax = st.text_input('Annual Tax:')\n",
        "    min_down = st.text_input('Minimum Down Payment Percentage:')\n",
        "    num_rooms = st.text_input('Number of Rooms:')\n",
        "    yr_built = st.text_input('Year Built:')\n",
        "    mnt_fee = st.text_input('Yearly Maintence Fee:')\n",
        "    story_num = st.text_input('Floor Number:')\n",
        "    school_data = None\n",
        "    #cat variables\n",
        "    unit_zip = st.selectbox('Zipcode: ', tuple([int(i) for i in list(u_cat_options['Zipcode'].dropna())]))\n",
        "    unit_bedrooms = st.selectbox('Number of Bedrooms: ', tuple(u_cat_options['# Bedrooms'].dropna()))\n",
        "    unit_unit_type = st.selectbox('Unit Type: ', tuple(u_cat_options['Unit Type'].dropna()))\n",
        "    unit_unit_building_type = st.selectbox('Building Type: ', tuple(u_cat_options['Building Type'].dropna()))\n",
        "    unit_unit_amenities = st.multiselect('Amenities: ', tuple(u_cat_options['Amentities'].dropna()))\n",
        "    #output\n",
        "    if st.button('Predict'):\n",
        "      unit_regression_variables = [float(num_bathrooms), float(sq_ft), float(days_on_market), float(annual_tax), float(min_down), float(num_rooms), float(yr_built), float(mnt_fee), float(story_num), school_data]\n",
        "      u_cat_answers = [int(unit_zip),unit_bedrooms,unit_unit_type,unit_unit_building_type,unit_unit_amenities]\n",
        "      st.write('Based off of your parameters, our model predicts that your unit is worth:')\n",
        "      st.write(units_predict(unit_regression_variables,u_cat_answers,units_model))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if choice == 'Rental Price Predictor':\n",
        "    folium_static(heatmapRentals)\n",
        "    folium_static(rentalChoropleth)\n",
        "    st.write('Input values to generate predicted price:\\n')\n",
        "    #regression variables\n",
        "    num_bathrooms = st.text_input('Number of Bathrooms:')\n",
        "    sq_ft = st.text_input('Square Footage:')\n",
        "    days_on_market = st.text_input('Days on Market:')\n",
        "    num_rooms = st.text_input('Number of Rooms:')\n",
        "    yr_built = st.text_input('Year Built:')\n",
        "    story_num = st.text_input('Floor Number:')\n",
        "    num_residences = st.text_input('Number of Units in Building')\n",
        "    school_data = None\n",
        "    #cat variables\n",
        "    rental_zip = st.selectbox('Zipcode:', zip_codes)\n",
        "    rental_unit_type = st.selectbox('Unit type:',unit_types)\n",
        "    rental_furnished = st.selectbox('Furnish status:',tuple(r_cat_options['Furnished'].dropna()))\n",
        "    rental_building_type = st.selectbox('Building type:',tuple(r_cat_options['Building Type'].dropna()))\n",
        "    rental_num_bedrooms = st.selectbox('Number of bedrooms:',tuple(r_cat_options['# Bedrooms'].dropna()))\n",
        "\n",
        "    values = list(r_cat_options['Rental Incentives'].dropna())\n",
        "    values += ['None']\n",
        "    rental_rental_incentives = st.selectbox('Rental incentives:',tuple(values))\n",
        "    if rental_rental_incentives == 'None':\n",
        "      rental_rental_incentives = None\n",
        "\n",
        "    rental_amentites = st.multiselect('Amenities:',tuple(r_cat_options['Amentities'].dropna()))\n",
        "    #output\n",
        "    if st.button('Predict'):\n",
        "      rental_regression_variables = [float(num_bathrooms), float(sq_ft), float(days_on_market), float(num_rooms), float(yr_built),float(story_num), float(num_residences), school_data]\n",
        "      r_cat_answers = [int(rental_zip),rental_unit_type,rental_furnished,rental_building_type,rental_num_bedrooms,rental_rental_incentives,rental_amentites]\n",
        "      st.write('Based off of your parameters, our model predicts that your rental cost each month is:')\n",
        "      st.write(rentals_predict(rental_regression_variables,r_cat_answers,rentals_model))\n",
        "\n",
        "\n",
        "  if choice == 'Building Price Predictor':\n",
        "    folium_static(heatmapBuildings)\n",
        "    folium_static(buildingChoropleth)\n",
        "    st.write('Input values to generate predicted price:\\n')\n",
        "    #regression variables\n",
        "    street_num = st.text_input('Street number:')\n",
        "    street_name = st.text_input('Street name:')\n",
        "    if st.button('Predict'):\n",
        "      bbl = fetch_bbl(street_num, street_name, None)\n",
        "      st.write('Based off of your parameters, our model predicts that your building is worth:')\n",
        "      st.write(int(buildings_predict(bbl,engine,building_model)))\n",
        "\n",
        "  if choice == 'About':\n",
        "    st.subheader(\"Created by: \\nJack Roberts, Zach Nemeth, Marco Espaillat, Maxwell Yan, Akhil Vajjhala\")\n",
        "    st.write(\"Divinus aims to sort through current and past real estate data in order to provide users an informative visualization of the current real estate market within Manhattan. Through this project, users would have access to potential investment opportunities, a visualization of over/under priced neighborhoods, and an estimated value for a distinct property value. Divinus aims to provide three main interfaces. To start, users would have access to a heat map of Manhattan illustrating a color coded representation of mean prices. Secondly, users would have access to a predictive estimator outputting our algorithm’s suggested price for your property. Finally, users would see an area where properties are underpriced yet rentals are overpriced in order to find high return investment properties.\")\n",
        "  \n",
        "  if choice == 'Data':\n",
        "    import plotly.express as px\n",
        "    #fig = px.scatter(df_rake, x=\"Word Frequency\", y=\"Word Degree\", log_x=False, hover_name=\"Phrase\", hover_data=[\"Word Ratio\"])\n",
        "    #figRentals = px.scatter(df_rakeRentals, x=\"Word Frequency\", y=\"Word Degree\", log_x=False, hover_name=\"Phrase\", hover_data=[\"Word Ratio\"])\n",
        "\n",
        "\n",
        "    st.write(\"Analysis of Data\")\n",
        "    st.write('\\nML Number of Houses in Zipcode')\n",
        "    st.image('houses_Zipcodes.png')\n",
        "    st.write('\\n Units by District')\n",
        "    st.image('units_by_district.png')\n",
        "    st.write('\\nNYXRSA Index vs Time')\n",
        "    st.image('index_data1.png')\n",
        "    st.write('\\nML SHAP Value(Unit)')\n",
        "    st.image(['UnitHist.png','UnitPlot.png'])\n",
        "    st.write('\\nML SHAP Value(Rental)')\n",
        "    st.image(['RentalHist.png','RentalPlot.png'])\n",
        "    st.write('\\nML SHAP Value(Building)')\n",
        "    st.image(['BuildingHist.png','BuildingPlot.png'])\n",
        "    st.write('\\nRAKE Keyword Frequency vs Degree Scatterplot(Unit)')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "    st.write('\\nRAKE Keyword Frequency vs Degree Scatterplot(Rental)')\n",
        "    st.plotly_chart(figRentals, use_container_width=True)\n",
        "    st.write('\\nNGram Frequency Bar Chart(Unit)')\n",
        "    st.image(['unigramsUnits.PNG','bigramsUnits.PNG','trigramsUnits.PNG'], width = 450)\n",
        "    st.write('\\nNGram Frequency Bar Chart(Rental)')\n",
        "    st.image(['unigramsRentals.PNG','bigramsRentals.PNG','trigramsRentals.PNG'],  width = 450)\n",
        "\n",
        "  def zach_print(input_txt):\n",
        "    st.write(input_txt + '\\n')\n",
        "  \n",
        "  if choice == 'Unit Listing Description Analysis':\n",
        "    nlp_input = st.text_input('Enter Listing Description:')\n",
        "    if st.button('Analyze'):\n",
        "      st.write(listingDescriptionAdvisorUnits(nlp_input,zach_print))\n",
        "      st.image('wordCloudUnits.png', width = 750)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "  if choice == 'Rental Listing Description Analysis':\n",
        "    nlp_input = st.text_input('Enter Listing Description:')\n",
        "    if st.button('Analyze'):\n",
        "      st.write(listingDescriptionAdvisorRentals(nlp_input,zach_print))\n",
        "      st.image('wordCloudUnits.png', width = 750)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naw7yFs907_1"
      },
      "source": [
        "!streamlit run app.py &>/dev/null &"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6-WoZ8t0_VA",
        "outputId": "99fefac0-c46b-4b56-e939-bce342df9eca"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://6bbb692ac03c.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly9Vz9edusRp"
      },
      "source": [
        "# If ngrok isn't working use this.\n",
        "#!npm install localtunnel\n",
        "#!npx localtunnel --port 8501"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvpfe2kd1ET6"
      },
      "source": [
        "ngrok.kill()\n",
        "!pkill streamlit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}